\documentclass[handout]{beamer}

\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}

\usepackage{verbatim,xmpmulti,color,multicol,multirow}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\graphicspath{{../figs/}}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}

\title[MCMCEM for RNAseq]{Monte Carlo Expectation Maximization for RNAseq}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\begin{document}

%\section{Temp??} \begin{comment}

<<chunk_options, echo=FALSE, message=FALSE>>=
opts_chunk$set(fig.width=6, fig.height=5, out.width='.8\\linewidth', fig.align='center', size='tiny')
library(plyr)
library(ggplot2)
library(reshape2)
library(rstan)
@

\frame{\titlepage}

\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item EM
\end{itemize}
\end{frame}



\section{Expectation maximization}
\subsection{Hierarchical models}
\begin{frame}
\frametitle{Hierarchical models}
Generically 
\[ y|\theta \sim p(y|\theta) \qquad \theta|\pi \sim p(\theta|\pi) \]

\vspace{0.2in} \pause 

Conditional independencies:
\begin{itemize}[<+->]
\item Data model 
\[ p(y|\theta) = \prod_{g=1}^G p(y_g|\theta_g) \]
\item Hierarchical distribution
\[ p(\theta|\pi) = \prod_{g=1}^G p(\theta_g|\pi) \]
\end{itemize}
where $G$ is large, e.g. $\sim 10^4$. 
\end{frame}




\begin{frame}
\frametitle{Expectation-Maximization algorithm}

Suppose we wish to find 
\[ \hat{\pi}_{MLE} = \mbox{argmax}_\pi p(y|\pi) \quad \mbox{where} \quad p(y|\pi) = \int p(y|\theta) p(\theta|\pi) d\theta \]

\pause

An expectation-maximization algorithm is 
\begin{enumerate}
\item Expectation: 
\[ \begin{array}{rll}
Q(\pi|\pi^{(t)}) &= E_{\theta|y,\pi^{(t)}}\left[\log L(\pi|y,\theta) \right] \pause \\
& = E_{\theta|y,\pi^{(t)}}\left[\log p(y|\theta)\right] &+ E_{\theta|y,\pi^{(t)}}\left[\log p(\theta|\pi)\right] \pause \\
& = C &+ E_{\theta|y,\pi^{(t)}}\left[\log p(\theta|\pi)\right] 
\end{array} \]
\item \pause Maximization:
\[ \pi^{(t+1)} = \mbox{argmax}_{\pi} Q(\pi|\pi^{(t)}) \pause = \mbox{argmax}_{\pi} E_{\theta|y,\pi^{(t)}}\left[\log p(\theta|\pi)\right]  \]
\end{enumerate}

\pause

Theory states that 
\[ \pi^{(t)} \to \hat{\pi}_{MLE} \quad \mbox{as } t\to\infty. \]

\end{frame}


\begin{frame}
\frametitle{Normal with hierarchical mean}

Let $y_{gi} \stackrel{ind}{\sim} N(\theta_g, s_g^2)$ and $\theta_g \sim N(\mu,\tau)$ for $i=1,\ldots,n_g=n$ and $g=1,\ldots,G$. \pause Let $\pi=(\mu,\tau)$,  
\[ \begin{array}{rl}
m_g^{(t)} &= E\left[\theta_g\left|y,\pi^{(t)}\right.\right] = v_g^{(t)}\left[\frac{\mu^{(t)}}{\tau^{(t)}} + \frac{\overline{y}_g}{s_g^2/n_g} \right] \\
v_g^{(t)} &= V\left[\theta_g\left|y,\pi^{(t)}\right.\right] = \left[\frac{1}{\tau^{(t)}} + \frac{1}{s_g^2/n_g} \right]^{-1}
\end{array} \]


Then, we have 
{\small
\[ \begin{array}{rl}
E_{\theta\left|y,\pi^{(t)}\right.}\left[\log p(\theta|\pi)\right]
%&= \log \prod_{g=1}^G (2\pi \tau)^{-1/2} \exp\left(-\frac{1}{2\tau} (\theta_g-\mu)^2 \right) \\
%&= \sum_{g=1}^G \left[ -\log(2\pi \tau)/2  -\frac{1}{2\tau} (\theta_g-\mu)^2\right] \\
&= -\frac{G\log(2\pi \tau)}{2}-\frac{1}{2\tau}\sum_{g=1}^G  E_{\theta\left|y,\pi^{(t)}\right.}\left[(\theta_g-\mu)^2\right] \\
&= -\frac{G\log(2\pi \tau)}{2}-\frac{1}{2\tau}\sum_{g=1}^G v_g^{(t)} - \frac{1}{2\tau}\sum_{g=1}^G (m_g^{(t)}-\mu)^2 \\
\end{array} \]
}
\pause
which is maximized when 
{\small
\[ \begin{array}{rl}
\mu^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G m_g^{(t)} \pause \\ 
\tau^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G  E_{\theta\left|y,\pi^{(t)}\right.}\left[(\theta_g-\mu)^2\right] =  \frac{1}{G} \sum_{g=1}^G \left[ v_g^{(t)} + \left(m_g^{(t)}-\mu^{(t+1)}\right)^2 \right]. 
\end{array}\]
}
\end{frame}


\begin{frame}[fragile]

<<normal_hierarchical_mean_em, cache=TRUE, echo=FALSE>>=
G = 100
truth = data.frame(mu = 0, tau = 1)
d = ddply(data.frame(gene=1:G, theta = with(truth, rnorm(G, mu, sqrt(tau)))),
          .(gene), function(x) data.frame(gene=x$gene, y=rnorm(4, x$theta)))
sm = ddply(d, .(gene), summarize, n = length(y), ybar = mean(y))
mu = tau = 1 # Initial values
n_iter = 10
keep = data.frame(iteration = 1:n_iter, mu = rep(NA, n_iter), tau = rep(NA, n_iter), method="em")
for (i in 1:n_iter) {
  sm$v = 1/(1/tau+sm$n)
  sm$m = sm$v * (mu/tau + sm$ybar*sm$n)
  mu = mean(sm$m)
  tau = mean(sm$v + (sm$m-mu)^2)
  keep$mu[i] = mu; keep$tau[i] = tau
}
@

<<, dependson='normal_hierarchical_mean_em', echo=FALSE>>=
ggplot(melt(keep, id.var=c("iteration","method"), variable.name="parameter"), 
       aes(iteration, value)) +
  geom_point() + 
  facet_wrap(~parameter, scales="free_y") +
  geom_hline(data=melt(truth, variable.name='parameter'), aes(yintercept=value), color='red')
@

\end{frame}


\subsection{Monte Carlo EM}
\begin{frame}
\frametitle{Monte Carlo EM}
If the expectation step is not analytically tractible, we can replace the expectation with a Monte Carlo approximation, \pause e.g. 
\[ 
E_{\theta|y,\pi^{(t)}}\left[\log p(\theta|\pi)\right] \approx \frac{1}{M} \sum_{m=1}^M \log p\left(\left.\theta^{(m)}\right|\pi\right)
\]
where $\theta^{(m)} \sim p(\theta|y,\pi^{(t)})$. \pause since
\[
\frac{1}{M} \sum_{m=1}^M \log p\left(\left.\theta^{(m)}\right|\pi\right) \to E_{\theta|y,\pi^{(t)}}\left[\log p(\theta|\pi)\right]
\]
as $m\to \infty$.
\end{frame}


\begin{frame}
\frametitle{Parallelism in hierarchical models}

In hierarchical models, we have 
\[ p\left(\theta\left|y,\pi^{(t)}\right.\right) = \prod_{g=1}^G p\left(\theta_g\left|y_g,\pi^{(t)}\right.\right)\]
and thus we can simulate $\theta_g$ independently across $g$. \pause Also, since we have 
\[ E_{\theta|y,\pi^{(t)}}[\log p(\theta|\pi)] = \sum_{g=1}^G E_{\theta|y,\pi^{(t)}}[\log p(\theta_g|\pi)] = \sum_{g=1}^G E_{\theta_{\alert{g}}|y,\pi^{(t)}}[\log p(\theta_g|\pi)] \]
we can approximate the expectation via 
\[
E_{\theta|y,\pi^{(t)}}[\log p(\theta|\pi)] \approx \frac{1}{M} \sum_{m=1}^M \sum_{g=1}^G \log p\left(\left.\theta_g^{(m)}\right|\pi\right).
\]

\end{frame}

\begin{frame}
\frametitle{Normal with hierarchical mean}

The Monte Carlo approximation to the desired expectation is

{\small
\[ \begin{array}{rl}
E_{\theta|y,\pi^{(t)}}[\log p(\theta|\pi)] &= -\frac{G\log(2\pi \tau)}{2}-\frac{1}{2\tau}\sum_{g=1}^G  E_{\theta_g\left|y,\pi^{(t)}\right.}\left[(\theta_g-\mu)^2\right] \\
&\approx -\frac{G\log(2\pi \tau)}{2}-\frac{1}{2\tau}\sum_{g=1}^G \frac{1}{M} \sum_{m=1}^M (\theta_g^{(m)}-\mu)^2
\end{array} \]
}

is maximized with 

\[ \begin{array}{rl}
\mu^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G \frac{1}{M}\sum_{m=1}^M \theta_g^{(m)} \\
\tau^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G  \frac{1}{M}\sum_{m=1}^M\left(\theta_g^{(m)} - \mu^{(t+1)} \right)^2
\end{array} \]
\pause 
where $\theta_g^{(m)} \stackrel{ind}{\sim} p\left(\theta_g\left| y_g,\pi^{(t)}\right.\right)$.
\end{frame}


\begin{frame}[fragile]
<<normal_hierarchical_mean_mcem, dependson="normal_hierarchical_mean_em", cache=TRUE, echo=FALSE>>=
M = 1000
mu = tau = 1 # Initial values
keep2 = data.frame(iteration = 1:n_iter, mu = rep(NA, n_iter), tau = rep(NA, n_iter), method='mcem')
for (i in 1:n_iter) {
  sm$v = 1/(1/tau+sm$n)
  sm$m = sm$v * (mu/tau + sm$ybar*sm$n)
  draws = ddply(sm, .(gene), function(x) {
    data.frame(theta = rnorm(M, x$m, sqrt(x$v)))
  })
  mu = mean(draws$theta)
  tau = mean((draws$theta-mu)^2)
  keep2$mu[i] = mu; keep2$tau[i] = tau
}
@

<<dependson=c('normal_hierarchical_mean_em','normal_hierarchical_mean_mcem'), echo=FALSE>>=
ggplot(melt(rbind(keep,keep2), id.var=c("iteration","method"), variable.name="parameter"), 
       aes(iteration, value, color=method)) +
  geom_point() + 
  facet_wrap(~parameter, scales="free_y") +
  geom_hline(data=melt(truth, variable.name='parameter'), aes(yintercept=value))
@
\end{frame}


\subsection{MCMC EM}
\begin{frame}
\frametitle{Markov chain Monte Carlo EM}

It will not always be easy to sample from $p\left(\theta\left|y,\pi^{(t)}\right.\right)$, so we may want to use an MCMC approach to sample from this distribution. \pause The resulting MCMCEM algorithm is identical to the MCEM algorithm, i.e. 

\[ \begin{array}{rl}
\mu^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G \frac{1}{M}\sum_{m=1}^M \theta_g^{(m)} \\
\tau^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G  \frac{1}{M}\sum_{m=1}^M\left(\theta_g^{(m)} - \mu^{(t+1)} \right)^2
\end{array} \]
\pause where 
\[ \theta_g^{(m)} \sim p\left(\theta_g\left|y_g,\pi^{(t)}\right.\right) \]
but now $\theta_g^{(m)}$ and $\theta_g^{(m+1)}$ are not independent \pause (they are still independent across $g$). 

\end{frame}



\begin{frame}[fragile]
<<normal_hierarchical_mean_mcmcem, dependson="normal_hierarchical_mean_em", cache=TRUE, echo=FALSE, results='hide'>>=
normal_hierarchical_mean_model = "
data {
  int<lower=1> n;
  real y[n];
  real mu;
  real<lower=0> s;
  real<lower=0> tau;
}
parameters {
  real theta;
}
model {
  theta ~ normal(mu,sqrt(tau));
  y ~ normal(theta, s);
}
"

m = stan_model(model_code = normal_hierarchical_mean_model)

mu = tau = 1 # Initial values
keep3 = data.frame(iteration = 1:n_iter, mu = rep(NA, n_iter), tau = rep(NA, n_iter), method='mcmcem')
for (i in 1:n_iter) {
  # Run MCMC using rstan
  draws = ddply(d, .(gene), function(x) {
    s = sampling(m, data = list(n = length(x$y), y=x$y, mu=mu, s=1, tau=tau), iter=M/2)
    data.frame(theta = as.numeric(extract(s, 'theta')$theta))
  })
  
  mu = mean(draws$theta)
  tau = mean((draws$theta-mu)^2)
  keep3$mu[i] = mu; keep3$tau[i] = tau
}
@

<<dependson=c('normal_hierarchical_mean_em','normal_hierarchical_mean_mcem'), echo=FALSE>>=
ggplot(melt(rbind(keep,keep2,keep3), id.var=c("iteration","method"), variable.name="parameter"), 
       aes(iteration, value, color=method)) +
  geom_point() + 
  facet_wrap(~parameter, scales="free_y") +
  geom_hline(data=melt(truth, variable.name='parameter'), aes(yintercept=value))
@

\end{frame}


\section{Applications}
\begin{frame}
\frametitle{Negative binomial example}

Let's suppose $y_{gi} \stackrel{ind}{\sim} NB(\theta_g,\psi_g)$, i.e. 
\[ E[y_{gi}] = \eta_g = e^{\theta_g} \quad\mbox{and}\quad V[y_{gi}] = \eta_g + e^{\psi_g}\eta_g \]
\pause 
and we assume a hierarchical distribution for $\theta_g$ and $\psi_g$:
\[ 
\theta_g \stackrel{ind}{\sim} N(\mu_\theta,\tau_\theta) \quad\mbox{and, independently,}\quad \psi_g \stackrel{ind}{\sim} N(\mu_\psi, \tau_\psi).
\]
\end{frame}


\begin{frame}
\frametitle{MCMCEM for negative binomial hierarchical model}

Let $\pi=(\mu_\theta, \tau_\theta, \mu_\psi, \tau_\psi)$, then 
\[ 
E_{\theta,\psi|y,\pi^{(t)}}[\log p(\theta,\psi|\pi)] = E_{\theta|y,\pi^{(t)}}[\log p(\theta|\mu_\theta,\tau_\theta)] + E_{\psi|y,\pi^{(t)}}[\log p(\psi|\mu_\psi,\tau_\psi)]
 \]
 which is maximized with
\[ \begin{array}{rl}
\mu_\theta^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G \frac{1}{M}\sum_{m=1}^M \theta_g^{(m)} \\
\tau_\theta^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G  \frac{1}{M}\sum_{m=1}^M\left(\theta_g^{(m)} - \mu_\theta^{(t+1)} \right)^2 \\
\mu_\psi^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G \frac{1}{M}\sum_{m=1}^M \psi_g^{(m)} \\
\tau_\psi^{(t+1)} &= \frac{1}{G} \sum_{g=1}^G  \frac{1}{M}\sum_{m=1}^M\left(\psi_g^{(m)} - \mu_\psi^{(t+1)} \right)^2
\end{array} \]
\pause where 
\[ \theta_g^{(m)},\psi_g^{(m)} \sim p\left(\theta_g,\psi_g\left|y_g,\pi^{(t)}\right.\right) \]
via MCMC. 
\end{frame}



\begin{frame}[fragile]

<<negative_binomial_data, cache=TRUE>>=
G = 100
n = 4
truth = data.frame(mu_theta = 3, tau_theta = 1, mu_psi = 0, tau_psi = 1)
par = data.frame(gene=1:G, 
                 theta = with(truth, rnorm(G, mu_theta, sqrt(tau_theta))),
                 psi   = with(truth, rnorm(G, mu_psi, sqrt(tau_psi))))
d = ddply(par, .(gene), function(x) {
  data.frame(y = rnbinom(n, mu = exp(x$theta), size=1/exp(x$psi)))
})
@

<<negative_binomial_stan, cache=TRUE>>=
negative_binomial_model = "
data {
  int<lower=1> n;
  int<lower=0> count[n];
  real mu_theta;
  real<lower=0> tau_theta;
  real mu_psi;
  real<lower=0> tau_psi;
}
parameters {
  real theta;
  real psi;
}
model {
  theta ~ normal(mu_theta, sqrt(tau_theta));
  psi   ~ normal(mu_psi  , sqrt(tau_psi  ));
  count ~ neg_binomial_2_log(theta, 1/exp(psi));
}
"
nb_m = stan_model(model_code = negative_binomial_model)
@

<<negative_binomial_mcmcem, dependson=c('negative_binomial_data','negative_binomial_stan'), cache=TRUE, echo=FALSE, results='hide'>>=
mu_theta = 0; tau_theta=1; mu_psi = -2; tau_psi = 1 # Initial values
n_iter = 100
keep = data.frame(iteration = 1:n_iter, 
                  mu_theta = rep(NA, n_iter), 
                  tau_theta = rep(NA, n_iter), 
                  mu_psi = rep(NA,n_iter),
                  tau_psi = rep(NA,n_iter))
for (i in 1:n_iter) {
  # Run MCMC using rstan
  draws = ddply(d, .(gene), function(x) {
    s = sampling(nb_m, data = list(n = length(x$y), count=x$y, 
                                   mu_theta=mu_theta, tau_theta=tau_theta, 
                                   mu_psi=mu_psi, tau_psi=tau_psi))
    e = extract(s, c('theta','psi'))
    data.frame(theta = as.numeric(e$theta),
               psi = as.numeric(e$psi))
  })
  
  mu_theta = mean(draws$theta)
  tau_theta = mean((draws$theta-mu_theta)^2)
  mu_psi = mean(draws$psi)
  tau_psi = mean((draws$psi-mu_psi)^2)
  keep$mu_theta[i] = mu_theta; keep$tau_theta[i] = tau_theta
  keep$mu_psi[i] = mu_psi; keep$tau_psi[i] = tau_psi
}
@
\end{frame}

\begin{frame}[fragile]
<<dependson=c('normal_hierarchical_mean_em','normal_hierarchical_mean_mcem'), echo=FALSE>>=
ggplot(melt(keep, id.var=c("iteration"), variable.name="parameter"),
       aes(iteration, value)) +
  geom_point() + 
  facet_wrap(~parameter, scales="free_y") +
  geom_hline(data=melt(truth, variable.name='parameter'), aes(yintercept=value))
@


\end{frame}


\section{Heterosis}
\begin{frame}[fragile]
\frametitle{Heterosis}

<<>>=
heterosis_model = "
data {
  int<lower=1> n;
  int<lower=0> count[n];
  int<lower=1, upper=3> variety[n];
  real mu_phi;
  real mu_alpha;
  real mu_delta;
  real mu_psi;
  real<lower=0> tau_phi;
  real<lower=0> tau_alpha;
  real<lower=0> tau_delta;
  real<lower=0> tau_psi;
}
transformed data {
  matrix[n,3] X;
  for (i in 1:n) {
    if (variety[i] == 1) { X[i,1] <-  1; X[i,2] <- -1; X[i,3] <-  0; }
    if (variety[i] == 2) { X[i,1] <-  1; X[i,2] <-  1; X[i,3] <-  0; }
    if (variety[i] == 3) { X[i,1] <-  1; X[i,2] <-  0; X[i,3] <-  1; }
  }
}
parameters {
  real phi;
  real alpha;
  real delta;
  real psi;
}
transformed parameters {
  vector[3] pad;

  pad[1] <- phi;
  pad[2] <- alpha;
  pad[3] <- delta;
}
model {
  phi   ~ normal(mu_phi  , sqrt(tau_phi  ));
  alpha ~ normal(mu_alpha, sqrt(tau_alpha));
  delta ~ normal(mu_delta, sqrt(tau_delta));
  psi   ~ normal(mu_psi  , sqrt(tau_psi  ));
  count ~ neg_binomial_2_log(X*pad, 1/exp(psi));
}
"
h_m = stan_model(model_code = heterosis_model)
@
\end{frame}


<<heterosis_data, cache=TRUE>>=
G = 10
n = 4
truth = data.frame(mu_phi   = 3, tau_phi   = 1, mu_psi = 0, tau_psi = 1,
                   mu_alpha = 0, tau_alpha = 1, mu_delta = 0, tau_delta = .1^2)
par = data.frame(gene=1:G, 
                 phi   = with(truth, rnorm(G, mu_phi  , sqrt(tau_phi  ))),
                 alpha = with(truth, rnorm(G, mu_alpha, sqrt(tau_alpha))),
                 delta = with(truth, rnorm(G, mu_delta, sqrt(tau_delta))),
                 psi   = with(truth, rnorm(G, mu_psi  , sqrt(tau_psi  ))))
X = cbind(1, rep(c(-1,1,0), each=n), rep(c(0,0,1), each=n))
d = ddply(par, .(gene), function(x) {
  pad = with(x, c(phi,alpha,delta))
  data.frame(y = rnbinom(3*n, mu = exp(X%*%pad), size=1/exp(x$psi)),
             variety = rep(1:3, each=n))
})
@

<<heterosis_mcmcem, dependson=c('negative_binomial_data','negative_binomial_stan'), cache=TRUE, echo=FALSE, results='hide'>>=
current = as.list(truth)
n_iter = 10
keep = data.frame(iteration = 1:n_iter, 
                  mu_phi    = rep(NA, n_iter), 
                  mu_alpha  = rep(NA, n_iter), 
                  mu_delta  = rep(NA, n_iter), 
                  mu_psi    = rep(NA,n_iter),
                  tau_phi   = rep(NA, n_iter),
                  tau_alpha = rep(NA, n_iter), 
                  tau_delta = rep(NA, n_iter),
                  tau_psi   = rep(NA,n_iter))

for (i in 1:n_iter) {
  # Run MCMC using rstan
  draws = ddply(d, .(gene), function(x) {
    s = sampling(h_m, data = c(list(n = length(x$y), count=x$y, variety = x$variety), current))
    e = extract(s, c('phi','alpha','delta','psi'))
    data.frame(phi   = as.numeric(e$phi),
               alpha = as.numeric(e$alpha),
               delta = as.numeric(e$delta),
               psi   = as.numeric(e$psi))
  })
  
  current = list(mu_phi   = mean(draws$phi)  , tau_phi   = mean((draws$phi-mu_phi)^2),
                 mu_alpha = mean(draws$alpha), tau_alpha = mean((draws$alpha-mu_alpha)^2),
                 mu_delta = mean(draws$delta), tau_delta = mean((draws$delta-mu_delta)^2),
                 mu_psi   = mean(draws$psi)  , tau_psi   = mean((draws$psi-mu_psi)^2))
  keep$mu_phi[i]   = current$mu_phi;   keep$tau_phi[i]   = current$tau_phi
  keep$mu_alpha[i] = current$mu_alpha; keep$tau_alpha[i] = current$tau_alpha
  keep$mu_delta[i] = current$mu_delta; keep$tau_delta[i] = current$tau_delta
  keep$mu_psi[i]   = current$mu_psi;   keep$tau_psi[i]   = current$tau_psi
}
@
\end{frame}

\begin{frame}[fragile]
<<dependson=c('normal_hierarchical_mean_em','normal_hierarchical_mean_mcem'), echo=FALSE>>=
ggplot(melt(keep, id.var=c("iteration"), variable.name="parameter"),
       aes(iteration, value)) +
  geom_point() + 
  facet_wrap(~parameter, scales="free_y", nrow=2) +
  geom_hline(data=melt(truth, variable.name='parameter'), aes(yintercept=value))
@


\end{frame}




\end{document}
