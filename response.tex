\documentclass{article}

\usepackage{fullpage}
\usepackage{color}

\usepackage[parfill]{parskip}

\newcommand{\comment}[1]{\textit{#1}}
\newcommand{\response}[1]{#1}
\newcommand{\todo}[1]{{\color{red} #1}}

\begin{document}

\response{We thank the AE and two reviewers for their comments which dramatically improved the manuscript.}

Associate Editor comments:

\comment{The proposed model appears to be an extension of Ji et al. (2014) albeit with a differently specified likelihood function (negative binomial or Poisson-gamma) as perceived to be more appropriate for count data relative to the Gaussian likelihood function pursued in Ji et al. (2014). Both reviewers indicate that they are little underwhelmed by the performance of your proposed procedure relative to some of the existing methods that you compare against (i.e., ShrinkBayes, edgeR and baySeq).  Furthermore, they seem somewhat concerned about an ‘apples vs. oranges’ types of comparisons since the proposed hypothesis testing strategy in the proposed method is really not an option in the other methods.}

\response{We have assessed the Ji et al. approach and included it in Sections 3.2 and 3.3.}

\comment{Perhaps a reference to the promise of allele specific expression analyses (Discussion section) for inferring upon gene-expression heterosis might be useful to point readers into other directions with this work (Bell et al., 2013; Wei and Wang, 2013).}

\response{Added at the end of the Discussion section.}

\comment{I realize that Ji et al.’s (2014) work was intended for microarray data, or more specifically Gaussian likelihoods, but there has been some work that suggests that Gaussian likelihood specifications in MAANOVA may, oddly enough, have better properties for the analysis of RNA-seq data than more popular methods specifically intended for count data (edgeR and DESeq).  See Reeb and Steibel (2013) and also comments from one reviewer along these lines.  Since Ji et al.’s work also derives from ISU, have you assessed its performance relative to your proposed method on RNA-seq data?}

\response{We have included Ji et al.'s work as another alternative method, see Sections 3.2 and 3.3.} 

\comment{One seemingly glaring omission from this work is any comparative assessment of the proposed versus other methods (e.g. ShrinkBayes) on the actual maize dataset in Section 4.  Why is that missing?}

\response{We have provided a comparison of the computational time on the real data set using these methods.} \todo{What else should we do?}

\comment{The comments made by one reviewer regarding a relative assessment of computing time relative to the other methods deserves attention.}

\response{Added. See section 4.}

\comment{Page 1.  I was instructed that heterosis is merely characterized by the hybrid performance not being equal to the average of the two parental lines.  So in your very first sentence, I might write “Heterosis, or hybrid vigor, occurs when hybrid progeny display phenotypes that are superior to the average phenotypes of their parents.” i.e., I might be only interested in the $\delta_g$’s. However, your later characterization of heterosis is really something called overdominance to me…where the hybrid performance exceeds the performance of either parent.  I wonder if you could then be more specific in your definitions in your first sentence…and also in your third sentence in this particular paragraph. lines 48-59.  Perhaps this was already captured well in the Ji et al. (2014) paper but why not simply test heterosis as the difference between the hybrid and the average parental performance (i.e. $\mu_3 - 0.5 \mu_1 - 0.5 \mu_2$).  Again, I think your issue pertains really to an overdominance test of some sort.}

\response{Yes, overdominance is the scenario were are interested in. We have added the term in the introduction as well as throughout the manuscript to make this clear.}

\comment{Page 2 line 35.  “We CONSIDER an…”}

\response{Fixed.}

\comment{Page 3 Line 7.  I don’t it is sufficient to mention the statistical software “Stan”.  Please provide a suitable description and citation.}

\response{Added citation.}

\comment{Page 3 Line 26.  There are several ways to construct a Poisson-gamma distribution, all leading to different specifications of the relationship between the mean and variance of a negative binomial.  I apologize for the admittedly vain self-citation but there is one specification, for example, that specifies the variance to be a linear function of the mean of a negative binomial…see Equation 2.12b in  Tempelman and Gianola (1996).  I guess I’m not sure why most RNA-seq data analysts gravitate to a specification where the variance is specified to be linear and quadratic function of the mean as you do here.}

\response{We cannot answer why most RNAseq data analysts gravitate to this specification. We appreciate the reference and will consider the alternative specification in the future.}

\comment{Page 3. Bottom.  Shouldn’t you at least provide a justification when one would use a normal prior and when one would or should use a Laplace prior?}

\response{We have assessed the use of normal distributions see Section 3.3 and also provided rationale for why Laplace distributions would be expected to perform better in this scenario at the end of Section 2.1.}

\comment{Page 4, bottom half.  The strategy described here, also somewhat attested to by a reviewer, is a little disconcerting with respect to the “double estimation” of gene-specific parameters…first to estimate they hyperparameters and then again conditioned on the estimates of these hyperparameters.  Perhaps you should characterize that these second-stage gene-specific estimates are shrunk relative to those estimates used to estimate the hyperparameters and that they then have lead to predictions with better properties?  Ideally, I would have liked to see these hyperparameters be also formally estimated together with the gene specific parameters within one MCMC analyses…authors should address why they didn’t do this.}

\response{We have added a paragraph to the beginning of Section 2.2 to share our experience with a fully MCMC analysis which took too long and had poor mixing.}

\comment{Page 5, lines 37-47.  Since you’re doing MCMC, why not simply tabulate the number of times samples of $\mu_3$ exceeds either $\mu_1$ or $\mu_2$….and/or the number of times that $\mu_3$ is less than $\mu_1$ or $\mu_2$.  I suspect that what your test is really doing….in which case it might be useful to explicitly state this equivalence as such.}

\response{Yes, that is what the test is doing. We have now stated the case for LPH, i.e. ``$\delta_g < -|\alpha_g|$ (equivalently $\mu_{g3} < \min\{\mu_{g1},\mu_{g2}\}$).''

\comment{Page 6, top.  Is it possible then to assess your proposed method using Laplace versus normal priors to assess if that seems to be contributing to any differences between ShrinkBayes and your proposed method?  Maybe that is the real reason for the small improvements?...., and not the differences in sophistication of the constructed statistical tests?}

\response{We have assessed the use of normal distributions, see Section 3.3.}

\comment{Page 7, line 15.  Why delete the low counts?  Data quality issues?}

\response{ShrinkBayes failed for us when too many zeros were present. We do not believe the comparison of other methods would change if we did not remove the low counts.}

%\todo{During the course of our research, we were initially estimating hyperparameters based on moment matching of gene-specific parameters estimated using negative binomial regression independently for all genes. This procedure had issues whenever a gene-variety combination had counts that were all zeros, thus leading to an unidentified regression parameter, or too little variability, leading to an unidentified overdispersion parameter. This cut off appeared to fix the problem. None of the current methods suffer from this problem, in particular the edgeR based parameter estimations have a built shrinkage to estimate parameters. We did not change the simulations we were using and thus report the deletion of low counts for reproducible accuracy. We do not believe removing this deletion would change any of our results.}

\comment{Page 7, lines 17-22.  Can you provide us any information on how well the hyperparameters were estimated based on your empirical Bayes strategy?}

\response{Since we did not simulate data from our model, there are no true values of the hyperparameters and therefore we cannot directly compare our estimates to a truth.  We did try to point out in the Discussion that we believe these estimates could be improved.}

\comment{Page 8, line 37.  It is not obvious to me why $H_g2*$ (are the two parents the same?) is a necessary component of a heterosis (overdominance) test!?}

\response{It is not necessary, but it is sufficient. If parental expressions are the same and the hybrid is not, then the hybrid must be exhibiting overdominance.}

\comment{Page 9, lines 23-25.  In tandem with another reviewer’s comments, the real question is why?  Is it because of the suboptimality of the test that you contrived for ShrinkBayesA?}

\response{In Section 3.3, we highlighted the differences between ShrinkBayes and the eBayes approaches, but do not try to specifically state which difference is the cause.}

\comment{Page 10, Line 29.  Don’t you mean “nadir” rather than peak?}

\response{No, although we do mean ridge rather than peak, i.e. the count is high for effect size of zero and probabilities below 0.5.}

\comment{Page 11 line 39.  Is it really “independence” or nearly non-identifiability that is the issue?}

\response{We're sorry, but we don't understand this comment. We believe all the parameters in the model are identified and therefore are not sure what non-identifiability is being referenced.}

\comment{Page 11 last paragraph.  So this then begs an assessment of how good/poor these hyperparameter estimates were.}

\response{As mentioned above, as we have no true values of the hyperparameters, we cannot quantitatively assess the quality of the estimates.}

\comment{Page 11 last line “ASYMPTOTIC”}}

\response{Fixed.}

Reviewer \#1

\comment{The authors develop a hierarchical negative binomial model for drawing inferences from a composite null hypothesis, that tests gene expression heterosis. Simulations are performed to describe the performance of this new method along with a data analysis demonstrating an application to real data. This manuscript represents a well-conceived and executed methodological development in the statistical analysis of RNA-Seq gene expression data. However, the performance of the new method is underwhelming compared to existing methods, tempering my enthusiasm for the likely impact of this paper.}

\response{We assume that you mean the improvement compared to existing methods is underwhelming. To more accurately address the improvement, we calculated percentage improvement in AUC which ranges from 20\% to 100\% and discussed these in Section 3.3.}

\comment{(1) In the simulations, the authors should also compare the eBayes method to the method of Ji et al. (2014). Although developed for continuous microarray data, the authors could apply a simple transformation to the count data in order to make them continuous. I would be curious to know whether the eBayes method out-performs the method of Ji et al. (2014) on the transformed data.}

\response{We have assessed the approach of Ji et al. on transformed data, see Sections 3.2 and 3.3. The bottom line is that it doesn't perform as well as the methods based on our hierarchical negative-binomial model.}

\comment{(2) The authors should provide more details on the computational price of the eBayes method? How long (in comparison to edgeR, baySeq, ShrinkBayes) did the eBayes method take to run? Are these methods comparable in terms of computational cost?}

\response{Added. See section 4.}

\comment{(3) In the description of the simulation results, the eBayes and ShrinkBayes methods display very similar performance. The distinction between the results on the Figures is quite minimal. The authors should attempt to quantify the difference in performance in a single metric. For instance, in Figure 1, take the average percent difference in TPR across the FPRs that were used to generate the plot.}

\response{The area under the curve attempts to do exactly this, i.e. a single metric to compare the methods. We do not try to over-emphasize the difference between the ShrinkBayes and eBayes methods, which are both based on our model, as we do not believe there is much of a difference.}



Reviewer \#4

\comment{1. The authors propose a method to assess "gene expression heterosis."   While it is mentioned that this type of heterosis is a possible explanation for phenotypic heterosis, I'd like more discussion regarding this point.  The two references seem a bit outdated (for bioinformatics) so I'm wondering if this explanation is still being considered.  If it is, than I'd like to see more discussion about how the analytic results would be used to investigate this explanation.   Wouldn't scientists have specific phenotypic traits in mind and know of genes associated with these traits?  I'm a little confused why the search through the whole genome for this type of heterosis if they're looking to draw inference on the association.}

\response{Even though scientists may have specific candidates. An exploratory analysis to suggest other genes seems useful.} \todo{Updated references.}

\comment{2. The model appears reasonable and is adapted from on used with microarrays.  To support their approach, they simulate data using their model (ideal case) and find that their approach does slightly better than ShrinkBayes.}

\response{We simulate data from the negative-binomial portion of the model, but do not simulate from the hierarchical portion of the model. Instead, we use edgeR estimates for gene-specific parameters which provide some shrinkage for the overdispersion parameters but none for the other gene-specific parameters. We have tried to make this clear in Section 3.1. In addition, we have performed the analysis by simulating entirely from our model and achieve similar results.}

\response{We also tried to emphasize in Section 2 that ``our'' approach includes the implementation in ShrinkBayes as the model itself is novel and eBayes and ShrinkBayes are just two different approaches to estimation.}

\comment{a. How robust do you feel your method is?  There is no discussion along these lines.}

\todo{Add robustness discussion?}

\comment{b.      Why is there more of an improvement over ShrinkBayes with more replicates?  Is it because of the method used to calculate the posterior probabilities?  I'd like to see some discussion on this.}

\response{We have added discussion in Section 3.3. This discussion points out the two main differences between ShrinkBayes and the eBayes approaches, but does not try to state which is the cause because, frankly, we don't know.}

\comment{c.      The ROC curves are not particularly impressive.  Even with 16 reps, the true positive rate is below 50\% (controlling for the false positive rate).  The real data example included only 4 reps, which is far more likely a scenario.  Very little discussion is devoted to this.}

\response{We have added discussion at the end of Section 3.1. The bottom line is that the ROC curves are not particularly impressive because we did not use artificially large signals, but we feel this is likely representative of real data.}

\comment{3.      Minor edits or comments}

\comment{a.      Start of Section 2 on page 2  :  Replace "We considering" with just "Consider"}

\response{Fixed.}

\comment{b.      Page 3: "The parental averages and overdispersion parameters are assumed to follow normal distributions, i.e.,"}

\response{Fixed.}

\comment{c.      Right before Section 2.2…it is assumed apriori independence, correct?}

\response{Yes, added \emph{a priori}.}

\comment{d.      Page 4: You have a two-step approach where you estimate the parameters of interest to get estimates of the hyperparameters and then restimate.  How much change is there is these estimates?  Why aren't the methods of McCarthy used as a competitor?}

\response{We have added a figure (Fig 3) to compare gene-specific parameter estimates and a paragraphing describing the comparison in Section 4. We aren't exactly sure which McCarthy methods you are referring to. We already use an approach based on edgeR and the point estimates themselves do not naturally lead to a statistic for determining the plausibility of heterosis.}

\comment{e.      Page 11 "These figures show marked departures from ……"}

\response{Fixed.}

\comment{f.      Page 11 : Does it really matter that the posterior is different from the prior?  One often assume aprior independence but expects there to be posterior correlation.  Perhaps I'm just missing something in this discussion.}

\response{Yes, we believe it does matter. Due to the nature of an empirical Bayes analysis, we only learn about the joint distribution of the gene-specific parameters through the hyperparameters, i.e. the gene-specific parameters will still be assumed to be independent and from the marginal distribution that was assumed (normal or Laplace). As an example, if the expression level ($\phi$) and overdispersion ($\psi$) parameters were assumed to have a bivariate normal distribution, then when inferring about $\psi$ we would use information about $\phi$, i.e. larger values of $\phi$ lead, on average, to smaller values for $\psi$. But this does not occur in the current model.}

\end{document}
