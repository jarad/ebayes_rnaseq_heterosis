\documentclass{article}

\usepackage{amsmath}

\newcommand{\argmax}{\mbox{argmax}}

\begin{document}

\section{MAP estimate}

Let $\theta_g$ be the gene-specific parameters and $\pi$ be the hyperparameters. The MAP estimate for $\pi$ is 
\begin{align*}
\argmax_{\pi} \log p(\pi|y) 
&= \argmax_{\pi} \log\left( \int \prod_{g=1}^G \left[ \, \prod_{v=1}^V \prod_{i=1}^{n_v} NB(y_{gvi}|f(\theta_g)) p(\theta_g|\pi) \right] p(\pi) d\theta \right) \\
&= \argmax_{\pi}\sum_{g=1}^G \log \left[ \, \int \prod_{v=1}^V \prod_{i=1}^{n_v} NB(y_{gvi}| f(\theta_g)) p(\theta_g|\pi) d\theta_g  \right] + \log(p(\pi)) \\
&= \argmax_{\pi}\sum_{g=1}^G \log(I_g) + \log(p(\pi)) 
\end{align*}
where 
 \[
 I_g = \int \prod_{v=1}^V \prod_{i=1}^{n_v} NB(y_{gvi}| f(\theta_g)) p(\theta_g|\pi) d\theta_g
 \]

\subsection{Quadrature}

Using quadrature, we would have a set of points $\theta_g^{(k)}$ that would depend on $\pi$ and associated weight $w_k$. We would approximate the integral via 
\[
I_g \approx \sum_{k=1}^K w_k \prod_{v=1}^V \prod_{i=1}^{n_v}  NB(y_{gvi}| f(\theta_g^{(k)})).
\]

\subsection{Monte Carlo integration}

Alternatively, we can use Monte Carlo integration which would sample $\theta_g^{(k)} \sim p(\theta_g|\pi)$ and approximate the integral via 
\[
I_g \approx \frac{1}{K} \sum_{k=1}^K \prod_{v=1}^V \prod_{i=1}^{n_v}  NB(y_{gvi}| f(\theta_g^{(k)})).
\]


\subsection{Importance sampling}

Importance sampling uses an alternative sampling distribution $q(\theta_g|\pi)$. Then we sample $\theta_g^{(k)} \sim q(\theta_g|\pi)$ and approximate the integral via 
\[
I_g \approx \frac{1}{K} \sum_{k=1}^K w_k \prod_{v=1}^V \prod_{i=1}^{n_v} NB(y_{gvi}| f(\theta_g^{(k)})).
\]
where $w_k = p(\theta^{(k)}|\pi)/q(\theta^{(k)}|\pi)$. 

If we make the proposal gene specific, i.e. $q_g(\theta_g|\pi)$, then we can sample using a proposal that is tuned to the data. For example, one component of $\theta_g$ is $\phi_g$, the mean parental response, i.e. $E[y_{g1i}+y_{g2i}]/2$. This parameter is extremely variable across $g$. It certainly makes sense to choose $\phi_g$ in a range near the empirical parental average, i.e. 

\[ \hat{\phi}_g = \frac{1}{2}\left[ \frac{1}{n_1} \sum_{i=1}^{n_1} y_{g1i} + \frac{1}{n_2} \sum_{i=1}^{n_2} y_{g2i} \right] \]


\section{Expectation-Maximization}

An EM algorithm to obtain MAP estimates for $\pi$ is 
\begin{enumerate}
\item Expectation
\[ Q(\pi|\pi^{(t)}) = E_{\theta|y,\pi^{(t)}}[\log p(\theta|y)] \]
\item Maximization
\[ \pi^{(t+1)} = \argmax_{\pi} Q(\pi|\pi^{(t)})  \]
\end{enumerate}

Below, the expectation is always with respect to $p(\theta|y,\pi^{(t)})$. 

The expectation can be broken up into its constitutive components, i.e. 

\[
E[\log p(\theta|y)]= E[C] + E[\log p(y|\theta)] + E[\log p(\theta|\pi)] + E[\log p(\pi)] 
\]

In the maximization step, we are maximizing with respect to $\pi$ and just $C + \log p(y|\theta)$ is irrelevant
if we are obtaining MLEs (instead of MAPs) then just ignore $\log p(\pi)$ as well. 

\subsection{MLEs} 

First consider just $E[\log p(\theta|\pi)]$ where 
\begin{equation}
p(\theta|\pi) = \prod_{g=1}^G p(\theta_g|\pi) = \prod_{g=1}^G p(\phi_g|\mu_\phi, \sigma_\phi) p(\alpha_g|\mu_\alpha, \sigma_\alpha) p(\delta_g|\mu_\delta, \sigma_\delta) p(\psi_g|\mu_\psi, \sigma_\psi) \label{e:theta}
\end{equation}

Thus, the expectation is 

\begin{align*}
E[\log p(\theta|\pi)]
=& E_{\theta|y,\pi^{(t)}} \left[ \sum_{g=1}^G \log p(\phi_g|\mu_\phi, \sigma_\phi) +\log p(\alpha_g|\mu_\alpha, \sigma_\alpha) +\log p(\delta_g|\mu_\delta, \sigma_\delta) +\log p(\psi_g|\mu_\psi, \sigma_\psi) \right] \\
=& E_{\phi|y,\pi^{(t)}} \left[ \sum_{g=1}^G \log p(\phi_g|\mu_\phi, \sigma_\phi) \right] +  E_{\alpha|y,\pi^{(t)}} \left[ \sum_{g=1}^G \log p(\alpha_g|\mu_\alpha, \sigma_\alpha) \right] \\
&+  E_{\delta|y,\pi^{(t)}} \left[ \sum_{g=1}^G \log p(\delta_g|\mu_\delta, \sigma_\delta) \right] +  E_{\psi|y,\pi^{(t)}} \left[ \sum_{g=1}^G \log p(\psi_g|\mu_\psi, \sigma_\psi) \right]
\end{align*}

\subsubsection{Normal model}

For simplicity, define the following 
\[
\hat{\phi}_{g}^{(t)} = E[\phi_g|y,\pi^{(t)}] \qquad \mbox{and} \qquad
\hat{\sigma}_{\phi_g}^{2(t)} = V[\phi_g|y,\pi^{(t)}] 
\]

For a normal model, e.g. $\phi_g|\pi \stackrel{ind}{\sim} N(\mu_\phi,\sigma_\phi^2)$, we have 
\[ 
\begin{array}{rl}
\multicolumn{2}{l}{
E_{\phi|y,\pi^{(t)}} \left[ \sum_{g=1}^G \log p(\phi_g|\mu_\phi, \sigma_\phi) \right] = 
E_{\phi|y,\pi^{(t)}} \left[ -\frac{G}{2}\log(2\pi \sigma_\phi^2) - \frac{1}{2\sigma_\phi^2} \sum_{g=1}^G (\phi_g-\mu_\phi)^2 \right]} \\
&= -\frac{G}{2}\log(2\pi \sigma_\phi^2) - \frac{1}{2\sigma_\phi^2} \sum_{g=1}^G E\left[\left(\phi_g-\mu_\phi\right)^2\right]\\
&= -\frac{G}{2}\log(2\pi \sigma_\phi^2) - \frac{1}{2\sigma_\phi^2} \sum_{g=1}^G E\left[\left(\phi_g-\hat{\phi}_{g}^{(t)}+\hat{\phi}_{g}^{(t)}-\mu_\phi\right)^2\right]\\
&= -\frac{G}{2}\log(2\pi \sigma_\phi^2) - \frac{1}{2\sigma_\phi^2} \sum_{g=1}^G 
E\left[\left(\phi_g-\hat{\phi}_{g}^{(t)}\right)^2+2\left(\phi_g-\hat{\phi}_{g}^{(t)}\right)\left(\hat{\phi}_{g}^{(t)}-\mu_\phi\right)+\left(\hat{\phi}_{g}^{(t)}-\mu_\phi\right)^2\right]\\
&= -\frac{G}{2}\log(2\pi \sigma_\phi^2) - \frac{1}{2\sigma_\phi^2} \sum_{g=1}^G \left[ \hat{\sigma}_{\phi_g}^{2(t)} + \left(\hat{\phi}_{g}^{(t)}-\mu_\phi\right)^2 \right]\\
&= -\frac{G}{2}\log(2\pi \sigma_\phi^2) - \frac{1}{2\sigma_\phi^2} \sum_{g=1}^G \hat{\sigma}_{\phi_g}^{2(t)} - \frac{1}{2\sigma_\phi^2} \sum_{g=1}^G \left(\hat{\phi}_{g}^{(t)}-\mu_\phi\right)^2 
\end{array}
\]


This is maximized when 
\[ \mu_\phi^{(t+1)} = \frac{1}{G} \sum_{g=1}^G \hat{\phi}_{g}^{(t)} \]
and
\[ 
\sigma_\phi^{2(t+1)} 
= \frac{1}{G} \sum_{g=1}^G E_{\phi|y,\pi^{(t)}}\left[\left.\left(\phi_g-\mu_\phi^{(t+1)}\right)^2\right|y,\pi^{(t)}\right] 
= \frac{1}{G} \sum_{g=1}^G \left[ \hat{\sigma}_{\phi_g}^{2(t)} + \left(\hat{\phi}_{g}^{(t)}-\mu^{(t+1)}_\phi\right)^2 \right].
\] 



\subsubsection{Laplace model}

For simplicity, define $\hat{\alpha}_{g}^{(t)}$ as a point estimate (median?) for $\alpha_g$ based on $\pi^{(t)}$.
% \[ \hat{\alpha}_{g}^{(t)} = E[\alpha_g|y,\pi^{(t)}] \qquad \mbox{and} \qquad
% \hat{\sigma}_{\alpha,g}^{2(t)} = V[\alpha_g|y,\pi^{(t)}]  \]

For a Laplace model, e.g. $\alpha_g|\pi \stackrel{ind}{\sim} La(\mu_\alpha,\sigma_\alpha)$, we have 
\[ 
\begin{array}{rl}
\multicolumn{2}{l}{
E_{\alpha|y,\pi^{(t)}} \left[ \sum_{g=1}^G \log p(\alpha_g|\mu_\alpha, \sigma_\alpha) \right] = 
E_{\alpha|y,\pi^{(t)}} \left[ -G\log(2\sigma_\alpha) - \frac{1}{\sigma_\alpha} \sum_{g=1}^G |\alpha_g-\mu_\alpha| \right]} \\
&= -G\log(2\sigma_\alpha) - \frac{1}{\sigma_\alpha} \sum_{g=1}^G E \left[|\alpha_g-\mu_\alpha| \right] \\
&\ge -G\log(2\sigma_\alpha) - \frac{1}{\sigma_\alpha} \sum_{g=1}^G E\left[\left|\alpha_g-\hat{\alpha}_{g}^{(t)}\right|\right] + \left|\hat{\alpha}_{g}^{(t)}-\mu_\alpha\right] \\
\end{array}
\]
Since the median minimizes absolute deviation and thus it is likely that 
\[ 
\mu_\alpha^{(t+1)} = \mbox{median}_g\, \hat{\alpha}_{g}^{(t)}
\]
Now the choice for what to use for $\hat{\alpha}_{g}^{(t)}$ appears pretty arbitrary. Perhaps we should use the posterior median since it will minimize $E\left[\left|\alpha_g-\hat{\alpha}_{g}^{(t)}\right|\right]$.

\[
\mu_\alpha^{(t+1)} = \frac{1}{G} \sum_{g=1}^G \hat{\alpha}_{g}^{(t)} \]
and 
\[
\sigma_\alpha^{2(t+1)} 
= \frac{1}{G} \sum_{g=1}^G E_{\alpha|y,\pi^{(t)}}\left[\left.\left|\alpha_g-\mu_\alpha^{(t+1)}\right|\right|y,\pi^{(t)}\right] 
\le  \frac{1}{G} \sum_{g=1}^G E_{\alpha|y,\pi^{(t)}}\left[\left|\alpha_g-\hat{\alpha}_{g}^{(t)}\right|\right] + \left|\hat{\alpha}_{g}^{(t)}-\mu^{(t+1)}_\alpha\right| .
\] 
We will probably just need to use a Monte Carlo approximation to this expectation. 



\end{document}
